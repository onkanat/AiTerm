#!/bin/zsh
# Smart Execute Multi-LLM Provider Module
# Bu dosya 癟oklu LLM salay覺c覺 desteini i癟erir

# LLM Provider yap覺land覺rmas覺
typeset -gA LLM_PROVIDERS=(
    "ollama" "http://localhost:11434/api/generate"
    "ollama_remote" "http://192.168.1.14:11434/api/generate"
    "openai" "https://api.openai.com/v1/chat/completions"
    "anthropic" "https://api.anthropic.com/v1/messages"
    "gemini" "https://generativelanguage.googleapis.com/v1beta/models"
)

typeset -gA LLM_MODELS=(
    "ollama" "${LLM_MODEL:-gemma3:1b-it-qat}"
    "ollama_remote" "devstral-small-2:latest"
    "openai" "gpt-3.5-turbo"
    "anthropic" "claude-3-sonnet-20240229"
    "gemini" "gemini-3.1-pro-preview"
)

# Sorgu karma覺kl覺覺n覺 deerlendirme
_assess_query_complexity() {
    local query="$1"
    local complexity="simple"
    
    # Karma覺kl覺k fakt繹rleri
    local word_count=$(echo "$query" | wc -w)
    [[ $word_count -gt 10 ]] && complexity="medium"
    [[ $word_count -gt 20 ]] && complexity="complex"
    
    # zel karakterler ve komutlar
    if [[ "$query" =~ '(\||\&\&|\;|grep|awk|sed|find)' ]]; then
        complexity="medium"
    fi
    if [[ "$query" =~ '(while|for|if|case|function|\$\(|\{\})' ]]; then
        complexity="complex"
    fi
    
    echo "$complexity"
}

# En uygun provider se癟imi
_select_best_provider() {
    local query="$1"
    local complexity=$(_assess_query_complexity "$query")
    
    # Varsay覺lan olarak mevcut provider'覺 kullan
    local selected_provider="${LLM_PROVIDER:-ollama}"
    
    # Eer Gemini API key varsa ve karma覺kl覺k y羹ksekse Gemini'yi se癟
    if [[ -n "${GEMINI_API_KEY:-}" ]]; then
        if [[ "$complexity" == "complex" || "$complexity" == "medium" ]]; then
            selected_provider="gemini"
            return 0
        fi
    fi

    # Fallback mant覺覺
    case "$complexity" in
        "simple")
            selected_provider="ollama"
            ;;
        "medium"|"complex")
            if [[ -n "${OPENAI_API_KEY:-}" ]]; then
                selected_provider="openai"
            else
                selected_provider="ollama"
            fi
            ;;
    esac
    
    echo "$selected_provider"
}

# Provider'a 繹zel API 癟ar覺s覺
_call_llm_provider() {
    local provider="$1"
    local prompt="$2"
    local mode="$3"
    
    case "$provider" in
        "ollama")     _call_ollama "$prompt" "$mode" "localhost" ;;
        "ollama_remote") _call_ollama "$prompt" "$mode" "192.168.1.14" ;;
        "openai")     _call_openai "$prompt" "$mode" ;;
        "anthropic")  _call_anthropic "$prompt" "$mode" ;;
        "gemini")     _call_gemini "$prompt" "$mode" ;;
        *)
            _audit_log "ERROR" "UNKNOWN_PROVIDER" "Provider: $provider"
            return 1
            ;;
    esac
}

# Gemini API 癟ar覺s覺 (Google One Pro / AI Studio)
_call_gemini() {
    local user_prompt="$1"
    local mode="$2"
    local model="${LLM_MODELS[gemini]}"
    local api_key="${GEMINI_API_KEY:-}"

    if [[ -z "$api_key" ]]; then
        _audit_log "ERROR" "GEMINI_NO_API_KEY" "API Key not found in environment"
        return 1
    fi

    # System prompt'u Gemini format覺na uyarla
    local system_instr="Sen Linux/macOS terminal uzman覺s覺n. Kullan覺c覺 isteklerini analiz edip SADECE JSON format覺nda yan覺t veriyorsun. JSON objesi tek sat覺rda olmal覺. T羹rk癟e karakterleri doru encode et. MODLAR: A) KOMUT MODU: {\"command\":\"ls -l\"} B) AIKLAMA MODU: {\"explanation\":\"A癟覺klama...\"}"
    
    local json_payload
    json_payload=$(jq -n \
        --arg sys "$system_instr" \
        --arg prompt "$user_prompt" \
        '{
            contents: [{ parts: [{ text: ($sys + "\n\n襤stek: " + $prompt) }] }],
            generationConfig: { response_mime_type: "application/json", temperature: 0.1 }
        }')

    curl -s --max-time $LLM_TIMEOUT \
         -X POST "${LLM_PROVIDERS[gemini]}/${model}:generateContent?key=${api_key}" \
         -H "Content-Type: application/json" \
         -d "$json_payload"
}

# Ollama API 癟ar覺s覺 (Local ve Remote destekli)
_call_ollama() {
    local user_prompt="$1"
    local mode="$2"
    local host="${3:-localhost}"
    local url="http://${host}:11434/api/generate"
    local model
    
    if [[ "$host" == "localhost" ]]; then
        model="${LLM_MODELS[ollama]}"
    else
        model="${LLM_MODELS[ollama_remote]}"
    fi
    
    local full_prompt="$SYSTEM_MESSAGE\n襤stek: $user_prompt"
    
    local json_payload
    json_payload=$(jq -n \
                   --arg model "$model" \
                   --arg prompt "$full_prompt" \
                   '{model: $model, prompt: $prompt, stream: false, format: "json"}')
    
    curl -s --max-time $LLM_TIMEOUT \
         -X POST "$url" \
         -H "Content-Type: application/json" \
         -d "$json_payload"
}

# Fallback mekanizmas覺 ile LLM 癟ar覺s覺
_call_llm_with_fallback() {
    local prompt="$1"
    local mode="$2"
    local primary_provider=$(_select_best_provider "$prompt")
    
    echo -n $'\n\e[2m LLM d羹羹n羹yor...\e[0m' >&2
    
    # S覺ras覺yla dene: Primary -> Local Ollama -> Remote Ollama
    local providers_to_try=("$primary_provider" "ollama" "ollama_remote")
    
    for provider in "${providers_to_try[@]}"; do
        [[ -z "$provider" ]] && continue
        
        _audit_log "LLM" "TRYING_PROVIDER" "Provider: $provider"
        
        local raw_response
        raw_response=$(_call_llm_provider "$provider" "$prompt" "$mode")
        
        if [[ $? -eq 0 && -n "$raw_response" ]]; then
            local response=$(_parse_provider_response "$provider" "$raw_response" "$mode")
            if [[ -n "$response" && "$response" != "null" && "$response" != "" ]]; then
                echo -ne "\r\e[K" >&2
                echo "$response"
                return 0
            fi
        fi
    done
    
    echo -ne "\r\e[K" >&2
    return 1
}

# Provider yan覺t覺n覺 parse etme
_parse_provider_response() {
    local provider="$1"
    local raw_response="$2"
    local mode="$3"
    local parsed_text
    
    case "$provider" in
        "gemini")
            parsed_text=$(echo "$raw_response" | jq -r '.candidates[0].content.parts[0].text // ""' 2>/dev/null)
            ;;
        "ollama"|"ollama_remote")
            parsed_text=$(echo "$raw_response" | jq -r '.response // ""' 2>/dev/null)
            ;;
        "openai")
            parsed_text=$(echo "$raw_response" | jq -r '.choices[0].message.content // ""' 2>/dev/null)
            ;;
        "anthropic")
            parsed_text=$(echo "$raw_response" | jq -r '.content[0].text // ""' 2>/dev/null)
            ;;
    esac
    
    # Fallback: if parsed_text is empty and raw_response is valid JSON error or just text
    if [[ -z "$parsed_text" ]]; then
        parsed_text="$raw_response"
    fi
    
    # JSON i癟inden as覺l veriyi 癟ek
    if echo "$parsed_text" | jq . >/dev/null 2>&1; then
        local content=""
        if [[ "$mode" == "explanation" ]]; then
            content=$(echo "$parsed_text" | jq -r '.explanation // .command // .response // empty' 2>/dev/null)
        else
            content=$(echo "$parsed_text" | jq -r '.command // .explanation // .response // empty' 2>/dev/null)
        fi
        
        if [[ -n "$content" && "$content" != "null" ]]; then
            echo "$content"
        else
            # JSON ama istenen alanlar yoksa ham text'i d繹n (veya t羹m JSON'覺)
            echo "$parsed_text"
        fi
    else
        # JSON deilse ham text'i d繹n
        echo "$parsed_text"
    fi
}
